<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>StartCam - Streamer</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background: #1a1a1a;
            color: white;
            text-align: center;
        }
        
        #localVideo {
            width: 100%;
            max-width: 640px;
            height: auto;
            border: 2px solid #333;
            border-radius: 8px;
            background: black;
        }
        
        button {
            padding: 12px 24px;
            margin: 10px;
            font-size: 16px;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            background: #007bff;
            color: white;
            transition: background 0.3s;
        }
        
        button:hover {
            background: #0056b3;
        }
        
        button:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }
        
        .status {
            margin: 20px 0;
            padding: 10px;
            border-radius: 4px;
            font-weight: bold;
        }
        
        .status.connected {
            background: #d4edda;
            color: #155724;
        }
        
        .status.error {
            background: #f8d7da;
            color: #721c24;
        }
        
        .status.info {
            background: #d1ecf1;
            color: #0c5460;
        }
        
        .volume-control {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            margin: 10px 0;
            color: white;
        }
        
        .volume-slider {
            width: 150px;
            height: 6px;
            background: #333;
            border-radius: 3px;
            outline: none;
            -webkit-appearance: none;
            appearance: none;
        }
        
        .volume-slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 18px;
            height: 18px;
            background: #007bff;
            border-radius: 50%;
            cursor: pointer;
        }
        
        .volume-slider::-moz-range-thumb {
            width: 18px;
            height: 18px;
            background: #007bff;
            border-radius: 50%;
            cursor: pointer;
            border: none;
        }
    </style>
</head>
<body>
    <h1>ðŸ“± StartCam Streamer</h1>
    <div id="status" class="status info">Ready to start streaming</div>
    
    <video id="localVideo" autoplay muted playsinline></video>
    
    <div>
        <button id="startBtn" onclick="startStream()">Start Camera</button>
        <button id="switchCameraBtn" onclick="switchCamera()" disabled style="display: none;">Switch Camera</button>
        <button id="muteBtn" onclick="toggleMute()" disabled>ðŸŽ¤ Mute</button>
        <div class="volume-control" id="volumeControl" style="display: none;">
            <span>ðŸ”Š</span>
            <input type="range" id="volumeSlider" class="volume-slider" min="0" max="200" value="100" oninput="adjustVolume(this.value)">
            <span id="volumeValue">100%</span>
        </div>
        <button id="connectBtn" onclick="connectToPeer()" disabled>Connect to Viewer</button>
    </div>

    <script src="https://unpkg.com/simple-peer@9.11.1/simplepeer.min.js"></script>
    <script>
        let localStream = null;
        let processedStream = null; // Stream with audio processing applied
        let audioContext = null;
        let gainNode = null;
        let peer = null;
        let ws = null;
        let currentCamera = 'user'; // 'user' for front, 'environment' for rear
        let availableCameras = [];
        let isMuted = false;
        let currentVolume = 100; // 0-200%
        
        const localVideo = document.getElementById('localVideo');
        const status = document.getElementById('status');
        const startBtn = document.getElementById('startBtn');
        const switchCameraBtn = document.getElementById('switchCameraBtn');
        const muteBtn = document.getElementById('muteBtn');
        const connectBtn = document.getElementById('connectBtn');
        const volumeControl = document.getElementById('volumeControl');
        const volumeSlider = document.getElementById('volumeSlider');
        const volumeValue = document.getElementById('volumeValue');
        
        function updateStatus(message, type = 'info') {
            status.textContent = message;
            status.className = `status ${type}`;
            console.log(`ðŸ“± [STREAMER] ${message}`);
        }
        
        // Add debug messages to page
        function addDebugMessage(message) {
            const debugDiv = document.getElementById('debug') || createDebugDiv();
            const timestamp = new Date().toLocaleTimeString();
            debugDiv.innerHTML += `<div>[${timestamp}] ${message}</div>`;
            debugDiv.scrollTop = debugDiv.scrollHeight;
            console.log(`ðŸ” [STREAMER DEBUG] ${message}`);
        }
        
        function createDebugDiv() {
            const debugDiv = document.createElement('div');
            debugDiv.id = 'debug';
            debugDiv.style.cssText = `
                position: fixed;
                top: 10px;
                right: 10px;
                width: 300px;
                height: 200px;
                background: rgba(0,0,0,0.8);
                color: lime;
                font-family: monospace;
                font-size: 10px;
                padding: 10px;
                border-radius: 5px;
                overflow-y: auto;
                z-index: 1000;
            `;
            document.body.appendChild(debugDiv);
            return debugDiv;
        }
        
        // Check available cameras
        async function detectCameras() {
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                availableCameras = devices.filter(device => device.kind === 'videoinput');
                console.log('ðŸ“± [STREAMER] Available cameras:', availableCameras.length);
                
                // Show switch button if multiple cameras available or on mobile
                const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
                if (availableCameras.length > 1 || isMobile) {
                    switchCameraBtn.style.display = 'inline-block';
                    console.log('ðŸ“± [STREAMER] Camera switching enabled');
                }
            } catch (error) {
                console.error('âŒ [STREAMER] Error detecting cameras:', error);
            }
        }
        
        async function startStream() {
            try {
                updateStatus('Requesting camera access...', 'info');
                console.log('ðŸŽ¥ [STREAMER] Requesting getUserMedia...');
                
                localStream = await navigator.mediaDevices.getUserMedia({
                    video: { 
                        width: { ideal: 1280 },
                        height: { ideal: 720 },
                        frameRate: { ideal: 30 },
                        facingMode: currentCamera
                    },
                    audio: true
                });
                
                console.log('ðŸŽ¥ [STREAMER] Got media stream:', localStream);
                console.log('ðŸŽ¥ [STREAMER] Video tracks:', localStream.getVideoTracks().length);
                console.log('ðŸŽ¥ [STREAMER] Audio tracks:', localStream.getAudioTracks().length);
                
                localStream.getTracks().forEach((track, index) => {
                    console.log(`ðŸ“Š [STREAMER] Track ${index}:`, {
                        kind: track.kind,
                        enabled: track.enabled,
                        readyState: track.readyState,
                        label: track.label
                    });
                });
                
                // Initialize audio processing (delay to avoid blocking camera)
                setTimeout(() => {
                    processedStream = initAudioProcessing();
                    
                    // Show volume control after audio processing is ready
                    if (processedStream && processedStream.getAudioTracks().length > 0) {
                        volumeControl.style.display = 'flex';
                        console.log('ðŸ”Š [STREAMER] Volume control enabled');
                    }
                }, 500);
                
                localVideo.srcObject = localStream;
                console.log('ðŸŽ¥ [STREAMER] Set video srcObject');
                
                // Wait for video to load
                localVideo.onloadedmetadata = () => {
                    console.log('ðŸŽ¥ [STREAMER] Local video metadata loaded:', {
                        videoWidth: localVideo.videoWidth,
                        videoHeight: localVideo.videoHeight,
                        duration: localVideo.duration
                    });
                    
                    // Check if OBS Virtual Camera is actually producing video
                    if (localVideo.videoWidth <= 2 || localVideo.videoHeight <= 2) {
                        console.warn('âš ï¸ [STREAMER] OBS Virtual Camera producing tiny video! Check OBS is running and camera is started.');
                    }
                    
                    // Verify tracks are producing data
                    localStream.getVideoTracks().forEach((track, index) => {
                        console.log(`ðŸŽ¬ [STREAMER] Video track ${index} settings:`, track.getSettings());
                        console.log(`ðŸ“Š [STREAMER] Video track ${index} constraints:`, track.getConstraints());
                    });
                };
                
                startBtn.disabled = true;
                switchCameraBtn.disabled = false;
                muteBtn.disabled = false;
                connectBtn.disabled = false;
                
                updateStatus('Camera started - Ready to connect', 'connected');
            } catch (err) {
                console.error('âŒ [STREAMER] Error accessing camera:', err);
                updateStatus('Error accessing camera: ' + err.message, 'error');
            }
        }
        
        // Switch between front and rear camera
        async function switchCamera() {
            if (!localStream) return;
            
            try {
                console.log('ðŸ“± [STREAMER] Switching camera from:', currentCamera);
                
                // Toggle camera
                currentCamera = currentCamera === 'user' ? 'environment' : 'user';
                console.log('ðŸ“± [STREAMER] Switching to:', currentCamera);
                
                // Stop current stream
                localStream.getTracks().forEach(track => track.stop());
                
                // Request new stream with different camera
                updateStatus('Switching camera...', 'info');
                
                localStream = await navigator.mediaDevices.getUserMedia({
                    video: { 
                        width: { ideal: 1280 },
                        height: { ideal: 720 },
                        frameRate: { ideal: 30 },
                        facingMode: currentCamera
                    },
                    audio: true
                });
                
                console.log('ðŸ“± [STREAMER] Camera switched successfully');
                
                // Reinitialize audio processing with new stream
                processedStream = initAudioProcessing();
                
                localVideo.srcObject = localStream;
                
                // Restore mute state after camera switch
                if (isMuted) {
                    localStream.getAudioTracks().forEach(track => {
                        track.enabled = false;
                    });
                    // Also mute processed stream if it exists
                    if (processedStream) {
                        processedStream.getAudioTracks().forEach(track => {
                            track.enabled = false;
                        });
                    }
                    console.log('ðŸŽ¤ [STREAMER] Restored muted state after camera switch');
                }
                
                // If connected to peer, update the stream
                if (peer && !peer.destroyed) {
                    console.log('ðŸ“± [STREAMER] Updating peer stream after camera switch');
                    
                    const streamToUse = processedStream || localStream;
                    const senders = peer._pc.getSenders();
                    
                    for (const sender of senders) {
                        if (sender.track && sender.track.kind === 'video') {
                            await sender.replaceTrack(streamToUse.getVideoTracks()[0]);
                            console.log('ðŸ“± [STREAMER] Replaced video track in peer connection');
                        } else if (sender.track && sender.track.kind === 'audio') {
                            await sender.replaceTrack(streamToUse.getAudioTracks()[0]);
                            console.log('ðŸŽµ [STREAMER] Replaced audio track in peer connection');
                        }
                    }
                }
                
                updateStatus('Camera switched - Ready to connect', 'connected');
            } catch (err) {
                console.error('âŒ [STREAMER] Error switching camera:', err);
                updateStatus('Camera switch failed: ' + err.message, 'error');
                
                // Try to restore previous camera
                currentCamera = currentCamera === 'user' ? 'environment' : 'user';
            }
        }
        
        // Toggle audio mute
        function toggleMute() {
            if (!localStream) return;
            
            const streamToMute = processedStream || localStream;
            const audioTracks = streamToMute.getAudioTracks();
            if (audioTracks.length === 0) {
                console.warn('âš ï¸ [STREAMER] No audio tracks to mute');
                return;
            }
            
            isMuted = !isMuted;
            
            // Toggle all audio tracks (both original and processed)
            localStream.getAudioTracks().forEach(track => {
                track.enabled = !isMuted;
            });
            if (processedStream) {
                processedStream.getAudioTracks().forEach(track => {
                    track.enabled = !isMuted;
                });
            }
            
            // Update button text and visual feedback
            muteBtn.textContent = isMuted ? 'ðŸ”‡ Unmute' : 'ðŸŽ¤ Mute';
            muteBtn.style.backgroundColor = isMuted ? '#dc3545' : '#007bff';
            
            // Update status
            const statusMsg = isMuted ? 'Microphone muted' : 'Microphone unmuted';
            console.log(`ðŸŽ¤ [STREAMER] ${statusMsg}`);
            updateStatus(statusMsg, isMuted ? 'error' : 'connected');
            
            // Show temporary status message
            setTimeout(() => {
                if (peer && !peer.destroyed) {
                    updateStatus('âœ… Connected to viewer!', 'connected');
                } else {
                    updateStatus('Camera started - Ready to connect', 'connected');
                }
            }, 2000);
        }
        
        function connectToPeer() {
            addDebugMessage('ðŸš€ connectToPeer() called');
            
            if (!localStream) {
                addDebugMessage('âŒ No localStream');
                updateStatus('Start camera first', 'error');
                return;
            }
            
            // Use processed stream if available, otherwise fallback to original
            const streamToUse = processedStream || localStream;
            addDebugMessage(`ðŸŽµ Using ${processedStream ? 'processed' : 'original'} stream`);
            
            addDebugMessage('âœ… localStream exists');
            
            // Prevent multiple connections
            if (peer && !peer.destroyed) {
                addDebugMessage('ðŸ”„ Destroying existing peer');
                peer.destroy();
            }
            if (ws && ws.readyState === WebSocket.OPEN) {
                addDebugMessage('ðŸ”„ Closing existing WebSocket');
                ws.close();
            }
            
            updateStatus('Connecting to signaling server...', 'info');
            addDebugMessage('ðŸ“¡ Creating WebSocket connection');
            
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${protocol}//${window.location.host}`;
            addDebugMessage(`ðŸ”— WebSocket URL: ${wsUrl}`);
            ws = new WebSocket(wsUrl);
            
            ws.onopen = () => {
                addDebugMessage('âœ… WebSocket connected');
                updateStatus('Creating peer connection...', 'info');
                addDebugMessage('ðŸ”§ Creating SimplePeer');
                
                try {
                    peer = new SimplePeer({
                        initiator: true,
                        trickle: true, // Enable trickle to get signals
                        stream: processedStream || localStream,
                        config: {
                            // Add STUN to force server reflexive candidates with actual IPs
                            iceServers: [
                                { urls: 'stun:stun.l.google.com:19302' }
                            ],
                            iceTransportPolicy: 'all',
                            iceCandidatePoolSize: 10,
                            rtcpMuxPolicy: 'require',
                            bundlePolicy: 'max-bundle'
                        }
                    });
                    addDebugMessage('âœ… SimplePeer created');
                } catch (error) {
                    addDebugMessage(`âŒ SimplePeer failed: ${error.message}`);
                    updateStatus('Peer creation failed: ' + error.message, 'error');
                    return;
                }
                
                
                peer.on('signal', (data) => {
                    addDebugMessage(`ðŸ“¤ Sending: ${data.type || 'unknown'}`);
                    
                    try {
                        ws.send(JSON.stringify({
                            type: 'signal',
                            data: data
                        }));
                        addDebugMessage(`âœ… Signal sent: ${data.type}`);
                        
                        // Let natural ICE candidates be sent via onicecandidate handler
                    } catch (error) {
                        addDebugMessage(`âŒ Signal failed: ${error.message}`);
                    }
                });
                
                addDebugMessage('ðŸ“¡ Signal handler attached');
                
                // Force offer creation if not automatic
                setTimeout(() => {
                    addDebugMessage('â° Checking for offer after 2s');
                    if (peer && peer._pc && peer._pc.signalingState === 'stable') {
                        addDebugMessage('ðŸ”¨ Forcing offer creation');
                        peer._pc.createOffer().then(offer => {
                            addDebugMessage('âœ… Manual offer created');
                            return peer._pc.setLocalDescription(offer);
                        }).then(() => {
                            addDebugMessage('âœ… Local description set');
                            addDebugMessage('â³ Waiting for answer...');
                        }).catch(err => {
                            addDebugMessage(`âŒ Manual offer failed: ${err.message}`);
                        });
                    } else {
                        addDebugMessage(`ðŸ” Peer state: ${peer._pc?.signalingState || 'unknown'}`);
                    }
                }, 2000);
                
                // Add timeout to force signal if needed
                setTimeout(() => {
                    console.log('â° [STREAMER] 5 second timeout - checking peer state');
                    console.log('ðŸ” [STREAMER] Peer destroyed?', peer.destroyed);
                    console.log('ðŸ” [STREAMER] Peer initiator?', peer.initiator);
                    console.log('ðŸ” [STREAMER] ICE gathering state:', peer._pc.iceGatheringState);
                    console.log('ðŸ” [STREAMER] Signaling state:', peer._pc.signalingState);
                }, 5000);
                
                peer.on('connect', () => {
                    console.log('ðŸ¤ [STREAMER] Peer data channel connected!');
                    updateStatus('âœ… Connected to viewer!', 'connected');
                    connectBtn.disabled = true;
                    
                    // Comprehensive WebRTC sender verification
                    const activeStream = processedStream || localStream;
                    console.log('ðŸ” [STREAMER] Verifying stream after connect:', {
                        streamId: activeStream?.id,
                        tracks: activeStream?.getTracks().length,
                        peerStreams: peer._pc.getLocalStreams ? peer._pc.getLocalStreams().length : 'N/A',
                        usingProcessedAudio: !!processedStream
                    });
                    
                    // Check WebRTC senders
                    const senders = peer._pc.getSenders();
                    console.log('ðŸ“¤ [STREAMER] WebRTC senders:', senders.length);
                    senders.forEach((sender, index) => {
                        console.log(`ðŸ“º [STREAMER] Sender ${index}:`, {
                            track: sender.track?.kind,
                            trackEnabled: sender.track?.enabled,
                            trackState: sender.track?.readyState,
                            trackLabel: sender.track?.label
                        });
                    });
                    
                    // Verify tracks are still live
                    const activeStream = processedStream || localStream;
                    activeStream.getTracks().forEach((track, index) => {
                        console.log(`ðŸŽ¬ [STREAMER] Active stream track ${index} status:`, {
                            kind: track.kind,
                            enabled: track.enabled,
                            readyState: track.readyState,
                            muted: track.muted
                        });
                    });
                });
                
                peer.on('error', (err) => {
                    console.error('Peer error:', err);
                    updateStatus('Connection error: ' + err.message, 'error');
                });
                
                // Comprehensive WebRTC debugging
                peer._pc.oniceconnectionstatechange = () => {
                    console.log(`ðŸ”— [STREAMER] ICE connection state: ${peer._pc.iceConnectionState}`);
                    updateStatus(`ICE: ${peer._pc.iceConnectionState}`, 'info');
                    
                    if (peer._pc.iceConnectionState === 'connected') {
                        console.log(`âœ… [STREAMER] ICE CONNECTED! Streaming to viewer.`);
                        updateStatus('âœ… ICE Connected - Streaming!', 'connected');
                    } else if (peer._pc.iceConnectionState === 'completed') {
                        console.log(`âœ… [STREAMER] ICE COMPLETED! Stream fully established.`);
                        updateStatus('âœ… Stream established!', 'connected');
                    } else if (peer._pc.iceConnectionState === 'failed') {
                        console.log(`âŒ [STREAMER] ICE FAILED! Cannot reach viewer.`);
                        updateStatus('âŒ ICE failed - Cannot reach viewer', 'error');
                    } else if (peer._pc.iceConnectionState === 'checking') {
                        console.log(`ðŸ”„ [STREAMER] ICE CHECKING... Trying to connect to viewer.`);
                        updateStatus('ðŸ”„ Connecting to viewer...', 'info');
                    }
                    
                    // Check if we're connected but peer.on('connect') didn't fire
                    if (peer._pc.iceConnectionState === 'connected' || peer._pc.iceConnectionState === 'completed') {
                        console.log('âœ… [STREAMER] ICE connected! Checking senders...');
                        
                        // Verify senders immediately when ICE connects
                        const senders = peer._pc.getSenders();
                        console.log('ðŸ“¤ [STREAMER] WebRTC senders at ICE connect:', senders.length);
                        senders.forEach((sender, index) => {
                            console.log(`ðŸ“º [STREAMER] Sender ${index}:`, {
                                track: sender.track?.kind,
                                trackEnabled: sender.track?.enabled,
                                trackState: sender.track?.readyState,
                                trackLabel: sender.track?.label?.substring(0, 20) + '...'
                            });
                        });
                        
                        // Check if tracks are still good
                        const activeStream = processedStream || localStream;
                        console.log('ðŸŽ¬ [STREAMER] Active stream status at ICE connect:', {
                            active: activeStream.active,
                            tracks: activeStream.getTracks().length,
                            usingProcessedAudio: !!processedStream
                        });
                        activeStream.getTracks().forEach((track, index) => {
                            console.log(`ðŸŽµ [STREAMER] Track ${index}:`, {
                                kind: track.kind,
                                enabled: track.enabled,
                                readyState: track.readyState,
                                muted: track.muted
                            });
                        });
                    }
                };
                
                peer._pc.onicegatheringstatechange = () => {
                    console.log(`ðŸ“¡ [STREAMER] ICE gathering state: ${peer._pc.iceGatheringState}`);
                    
                    // When gathering completes, start polling connection state
                    if (peer._pc.iceGatheringState === 'complete') {
                        console.log(`ðŸ” [STREAMER] Starting ICE state monitoring...`);
                        
                        const pollICEState = () => {
                            console.log(`ðŸ“Š [STREAMER] Current states:`, {
                                iceConnection: peer._pc.iceConnectionState,
                                iceGathering: peer._pc.iceGatheringState,
                                connection: peer._pc.connectionState,
                                signalingState: peer._pc.signalingState
                            });
                        };
                        
                        // Poll every 2 seconds for 10 seconds to see what's happening
                        pollICEState(); // immediate
                        setTimeout(pollICEState, 2000);
                        setTimeout(pollICEState, 4000);
                        setTimeout(pollICEState, 6000);
                        setTimeout(pollICEState, 8000);
                        setTimeout(pollICEState, 10000);
                    }
                };
                
                peer._pc.onconnectionstatechange = () => {
                    console.log(`ðŸ”Œ [STREAMER] Connection state: ${peer._pc.connectionState}`);
                };
                
                peer._pc.onicecandidate = (event) => {
                    if (event.candidate) {
                        addDebugMessage(`ðŸ§Š Local candidate: ${event.candidate.address}:${event.candidate.port}`);
                        console.log(`ðŸ§Š [STREAMER] ICE candidate:`, event.candidate.address);
                        
                        // Force replace any IP with routable VPN IP
                        if (event.candidate.address && !event.candidate.address.endsWith('.local')) {
                            addDebugMessage(`ðŸ”§ Replacing ${event.candidate.address} with VPN IP 10.25.200.50`);
                            
                            // Create candidate with VPN IP but keep original port and other details
                            const vpnCandidate = event.candidate.candidate.replace(
                                event.candidate.address,
                                '10.25.200.50'
                            );
                            
                            try {
                                ws.send(JSON.stringify({
                                    type: 'signal',
                                    data: {
                                        type: 'candidate',
                                        candidate: {
                                            candidate: vpnCandidate,
                                            sdpMLineIndex: event.candidate.sdpMLineIndex,
                                            sdpMid: event.candidate.sdpMid
                                        }
                                    }
                                }));
                                addDebugMessage(`âœ… VPN IP candidate sent: 10.25.200.50:${event.candidate.port}`);
                            } catch (error) {
                                addDebugMessage(`âŒ Failed to send VPN candidate: ${error.message}`);
                            }
                        } else if (event.candidate.address && event.candidate.address.endsWith('.local')) {
                            addDebugMessage(`ðŸš« Skipping .local candidate`);
                        }
                    } else {
                        addDebugMessage('âœ… ICE gathering complete');
                        console.log(`âœ… [STREAMER] ICE gathering complete`);
                    }
                };
            };
            
            ws.onmessage = async (event) => {
                try {
                    let data = event.data;
                    
                    // Handle Blob data
                    if (data instanceof Blob) {
                        data = await data.text();
                    }
                    
                    const message = JSON.parse(data);
                    console.log(`ðŸ“¥ [STREAMER] Received message type:`, message.type);
                    console.log(`ðŸ“¥ [STREAMER] Full message:`, message);
                    
                    if (message.type === 'signal' && peer) {
                        console.log(`ðŸ“‹ [STREAMER] Signal data type:`, message.data?.type);
                        
                        if (message.data.type === 'answer') {
                            addDebugMessage('ðŸ“‹ Got answer from viewer');
                            console.log(`ðŸ“‹ [STREAMER] Got answer! SDP length:`, message.data.sdp?.length);
                        } else if (message.data.candidate) {
                            // Extract IP from candidate string
                            const candidateString = message.data.candidate.candidate || message.data.candidate;
                            const ipMatch = candidateString.match(/(\d+\.\d+\.\d+\.\d+)\s+(\d+)/);
                            if (ipMatch) {
                                addDebugMessage(`ðŸŽ¯ Remote candidate: ${ipMatch[1]}:${ipMatch[2]}`);
                            }
                            console.log(`ðŸ§Š [STREAMER] Got ICE candidate:`, message.data.candidate);
                        }
                        
                        peer.signal(message.data);
                        console.log(`âœ… [STREAMER] Signal processed by peer`);
                    } else if (!peer) {
                        addDebugMessage('âš ï¸ No peer to handle signal');
                        console.log(`âš ï¸ [STREAMER] No peer object to handle signal`);
                    }
                } catch (error) {
                    console.error('âŒ [STREAMER] Error parsing WebSocket message:', error);
                    console.error('âŒ [STREAMER] Raw data:', event.data);
                }
            };
            
            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
                updateStatus('Signaling server error', 'error');
            };
            
            ws.onclose = () => {
                updateStatus('Disconnected from signaling server', 'error');
            };
        }
        
        // Audio processing functions
        function initAudioProcessing() {
            if (!localStream) return null;
            
            try {
                // Create audio context if it doesn't exist (with user interaction check)
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    console.log('ðŸŽµ [STREAMER] Created AudioContext');
                    
                    // Resume audio context if suspended (required on mobile)
                    if (audioContext.state === 'suspended') {
                        audioContext.resume().then(() => {
                            console.log('ðŸŽµ [STREAMER] AudioContext resumed');
                        }).catch(err => {
                            console.warn('âš ï¸ [STREAMER] Failed to resume AudioContext:', err);
                        });
                    }
                }
                
                // Get audio tracks
                const audioTracks = localStream.getAudioTracks();
                if (audioTracks.length === 0) {
                    console.warn('âš ï¸ [STREAMER] No audio tracks found');
                    return localStream;
                }
                
                // Create audio processing chain
                const source = audioContext.createMediaStreamSource(localStream);
                gainNode = audioContext.createGain();
                const destination = audioContext.createMediaStreamDestination();
                
                // Connect: microphone â†’ gainNode â†’ destination
                source.connect(gainNode);
                gainNode.connect(destination);
                
                // Set initial volume (100% = 1.0)
                gainNode.gain.value = currentVolume / 100;
                
                // Create new stream with processed audio + original video
                const videoTracks = localStream.getVideoTracks();
                const newStream = new MediaStream();
                
                // Add video tracks from original stream
                videoTracks.forEach(track => newStream.addTrack(track));
                
                // Add processed audio tracks
                destination.stream.getAudioTracks().forEach(track => newStream.addTrack(track));
                
                console.log('ðŸŽµ [STREAMER] Audio processing initialized:', {
                    originalAudioTracks: audioTracks.length,
                    processedAudioTracks: destination.stream.getAudioTracks().length,
                    videoTracks: videoTracks.length,
                    totalNewTracks: newStream.getTracks().length
                });
                
                return newStream;
            } catch (error) {
                console.error('âŒ [STREAMER] Audio processing failed:', error);
                return localStream; // Fallback to original stream
            }
        }
        
        function adjustVolume(value) {
            currentVolume = parseInt(value);
            volumeValue.textContent = currentVolume + '%';
            
            if (gainNode) {
                // Convert percentage to gain value (0-200% = 0.0-2.0)
                gainNode.gain.value = currentVolume / 100;
                console.log('ðŸ”Š [STREAMER] Volume adjusted to:', currentVolume + '%', 'gain:', gainNode.gain.value);
            }
        }
        
        // Initialize camera detection on page load
        window.addEventListener('load', () => {
            detectCameras();
        });
        
        window.addEventListener('beforeunload', () => {
            if (localStream) {
                localStream.getTracks().forEach(track => track.stop());
            }
            if (processedStream) {
                processedStream.getTracks().forEach(track => track.stop());
            }
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close();
            }
            if (peer) {
                peer.destroy();
            }
            if (ws) {
                ws.close();
            }
        });
    </script>
</body>
</html>